# 🚀 强化学习快速参考卡片

## 📌 核心公式速查

### 1. 回报 (Return)
```
G_t = r_t + γ*r_{t+1} + γ²*r_{t+2} + ...
```

### 2. 状态价值函数
```
V^π(s) = E_π[G_t | S_t = s]
```

### 3. 动作价值函数
```
Q^π(s,a) = E_π[G_t | S_t = s, A_t = a]
```

### 4. 优势函数
```
A^π(s,a) = Q^π(s,a) - V^π(s)
```

### 5. 策略梯度
```
∇_θ J(θ) = E_π[∇_θ log π_θ(a|s) * A^π(s,a)]
```

### 6. TD 误差
```
δ_t = r_t + γ*V(s_{t+1}) - V(s_t)
```

### 7. GAE (广义优势估计)
```
A_t^GAE = δ_t + (γλ)δ_{t+1} + (γλ)²δ_{t+2} + ...
```

### 8. PPO 裁剪目标
```
L^CLIP = E_t[min(r_t * A_t, clip(r_t, 1-ε, 1+ε) * A_t)]
```
其中 `r_t = π_new(a_t|s_t) / π_old(a_t|s_t)`

---

## 🎯 关键概念一句话

| 概念 | 一句话解释 |
|------|-----------|
| **状态 (State)** | 环境的当前情况 |
| **动作 (Action)** | 智能体可以执行的操作 |
| **奖励 (Reward)** | 环境对动作的反馈 |
| **策略 (Policy)** | 从状态到动作的映射 π(a\|s) |
| **价值函数 (Value)** | 从某个状态/动作出发的期望回报 |
| **优势 (Advantage)** | 动作相对于平均水平的优势 |
| **Episode** | 从开始到结束的完整轨迹 |
| **On-policy** | 只能用当前策略的数据更新 |
| **Off-policy** | 可以用旧策略的数据更新 |
| **GAE** | 平衡偏差和方差的优势估计方法 |
| **PPO** | 通过裁剪限制更新的策略优化算法 |

---

## 🔧 你的项目中的关键代码

### Actor (策略网络)
```python
mean, log_std = actor(state)  # 输出均值和标准差
action ~ N(mean, std²)        # 从正态分布采样
action = tanh(action_unbounded) * 2.0  # 限制到 [-2, 2]
```

### Critic (价值网络)
```python
value = critic(state)  # 输出 V(s)
```

### 优势计算 (GAE)
```python
delta = r_t + γ*V(s_{t+1}) - V(s_t)  # TD 误差
gae = delta + (γλ) * gae_{t+1}       # GAE 累加
advantages = gae                      # 优势
returns = advantages + values          # 用于训练 Critic
```

### PPO 更新
```python
ratio = π_new(a|s) / π_old(a|s)
surr1 = ratio * A
surr2 = clip(ratio, 1-ε, 1+ε) * A
actor_loss = -min(surr1, surr2) - entropy_coef * entropy
critic_loss = MSE(V(s), returns)
```

---

## 📊 超参数速查表

| 参数 | 典型值 | 作用 |
|------|--------|------|
| `gamma` | 0.99 | 未来奖励的折扣 |
| `gae_lambda` | 0.95 | GAE 的偏差-方差权衡 |
| `eps_clip` | 0.2 | PPO 裁剪范围 |
| `k_epochs` | 4 | 数据复用次数 |
| `lr_actor` | 1e-4 | Actor 学习率 |
| `lr_critic` | 3e-4 | Critic 学习率 |
| `entropy_coef` | 0.01 | 探索奖励系数 |
| `value_coef` | 1.0 | Critic 损失权重 |

---

## 🐛 常见问题排查

| 问题 | 可能原因 | 解决方法 |
|------|---------|---------|
| **训练不稳定** | 学习率太高 | 降低 lr_actor, lr_critic |
| **策略崩溃** | log_std 太小 | 限制 log_std 范围 (min=-2) |
| **NaN 值** | 梯度爆炸 | 梯度裁剪，检查数据 |
| **奖励不上升** | 探索不足 | 增加 entropy_coef |
| **训练太慢** | 样本效率低 | 增加 k_epochs，使用 GAE |
| **KL 散度太大** | 策略更新过大 | 减小学习率，增加 eps_clip |

---

## 📁 项目文件结构

```
step_01_explore_env.py      # 认识环境
step_02_env_wrapper.py      # 环境封装
step_04_actor.py            # Actor 网络
step_05_critic.py           # Critic 网络
step_06_collect_experience.py  # 经验收集
step_07_ppo_update.py       # PPO 更新逻辑
step_08_train.py            # 完整训练循环
step_09_evaluate.py         # 模型评估
```

---

## 🎓 学习路径

1. ✅ **理解 MDP**: 状态、动作、奖励、转移
2. ✅ **实现 Actor**: 策略网络，输出动作分布
3. ✅ **实现 Critic**: 价值网络，评估状态
4. ✅ **理解优势**: A(s,a) = Q(s,a) - V(s)
5. ✅ **实现 GAE**: 稳定优势估计
6. ✅ **实现 PPO**: 裁剪目标函数
7. ✅ **训练技巧**: 归一化、梯度裁剪、熵奖励
8. ✅ **稳定性**: NaN 检测、KL 监控

---

## 💡 记忆口诀

- **Actor 选动作，Critic 评价值**
- **优势 = 回报 - 价值**
- **GAE 平衡偏差和方差**
- **PPO 裁剪防过度更新**
- **熵奖励鼓励探索**
- **归一化减少方差**
- **梯度裁剪防爆炸**

---

**快速查阅，随时复习！** 📚

