# 倒立摆强化学习 · 分步学习路线

我们按下面步骤一点一点搭建，每步都会讲清楚「在学什么」和「代码在干什么」。

---

## 第一步：认识环境（当前）

- 用 Gymnasium 创建 Pendulum 环境
- 搞懂：**状态 (state)**、**动作 (action)**、**奖励 (reward)**
- 写一个最小脚本：`reset` → 循环 `step` → 打印信息

**目标**：能跑通环境，看懂每一维状态、动作、奖励的含义。

---

## 第二步：封装环境

- 写一个简单的 `PendulumWrapper` 类
- 统一 `reset` / `step` 的输入输出格式
- 为后续「状态维度、动作维度」等接口打基础

---

## 第三步：理解「策略」与「价值」

- 强化学习里：**策略 (Policy)** 负责选动作，**价值 (Value)** 负责评估好坏
- 用通俗例子理解：策略 = 怎么开车，价值 = 这条路有多好
- 暂时不写网络，只建立概念

---

## 第四步：实现简单策略网络 (Actor)

- 用 PyTorch 写一个很小的 MLP：`state → action`
- 先不管 PPO，只做「给定 state，输出一个 action」
- 在环境中跑几步，看看随机策略的表现

---

## 第五步：实现价值网络 (Critic)

- 再写一个 MLP：`state → value`
- 理解「价值」就是「从这个状态出发，期望能拿多少回报」
- 仍然不更新参数，只做前向传播

---

## 第六步：收集经验并理解 PPO

- 用当前策略在环境里跑，存 `(s, a, r, s', done)`
- 讲清楚：**为什么要用 PPO**、**on-policy / off-policy** 的直观区别
- 实现「存 transition」的逻辑，先不实现梯度更新

---

## 第七步：实现 PPO 更新

- 实现「目标函数」和「裁剪」的代码
- 把 Actor、Critic 的更新串起来
- 跑一个非常短的训练（几十个 episode），验证 loss 会动

---

## 第八步：完整训练循环

- 写 `train.py`：循环「收集经验 → PPO 更新 → 记录 reward」
- 加上简单画图（如 reward 曲线）
- 跑一段时间，看到倒立摆能立住

---

## 第九步：评估与可视化

- 加载训练好的模型，跑若干个 episode
- 可选：渲染、保存轨迹图、分析状态/动作曲线

---

**原则**：每一步都能单独运行、可观察结果；不提前写后面的复杂逻辑。

你完成**第一步**并理解后，告诉我，我们再一起写**第二步**。
