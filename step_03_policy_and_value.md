# 第三步：理解「策略」与「价值」

这一步**不写神经网络**，只把两个核心概念搞清楚，后面写 Actor / Critic 时才不会懵。

---

## 一、策略 (Policy) —— 「怎么做」

**定义**：给定当前**状态** \(s\)，告诉我该选哪个**动作** \(a\)。

- **输入**：`state`，比如 `[cos θ, sin θ, θ_dot]`
- **输出**：`action`，比如 `[-1.5]`（施加的力矩）

可以理解为：**策略 = 决策规则**。「看到这种情况，我决定这么干。」

**通俗比方**：
- 开车：策略 = 看到前面红灯 → 刹车；看到绿灯 → 加油。
- 倒立摆：策略 = 看到摆往左歪 → 往左施力；往右歪 → 往右施力。

**后面我们会用神经网络来近似策略**，那时就叫 **Actor**（演员）：它「表演」出该做的动作。

---

## 二、价值 (Value) —— 「有多好」

**定义**：给定当前**状态** \(s\)，估计**从此刻起，按当前策略玩下去，能拿到的总回报大概是多少**。

- **输入**：`state`
- **输出**：一个**实数**，表示「这个状态有多好」

**通俗比方**：
- 开车：价值 = 这条路堵不堵、安不安全、能多快到目的地。
- 下棋：价值 = 这个局面对自己赢面有多大。

**后面我们也会用神经网络来近似价值**，那时就叫 **Critic**（评论家）：它「评价」当前状态好不好。

---

## 三、它俩在强化学习里怎么用？

简化成一条链：

```
state ──→ Policy ──→ action ──→ env.step ──→ next_state, reward
   │
   └────→ Value ──→ "这个 state 大概能拿多少回报"
```

- **Policy**：负责**选动作**，和环境交互，拿到 `(s, a, r, s')` 这些数据。
- **Value**：负责**评估状态**，用来算「当前策略好不好」，从而指导 Policy 的更新。

我们后面用的 **PPO** 里：
- **Actor** = 策略网络，\(s \to a\)
- **Critic** = 价值网络，\(s \to \text{value}\)

训练时：用 Critic 算出来的「优势」等信息，来更新 Actor，让策略越变越好。

---

## 四、和第二步的关系

第二步我们写了 `PendulumWrapper`，有：

- `env.reset()` → `state`
- `env.step(action)` → `next_state, reward, terminated, truncated, info`
- `env.state_dim`, `env.action_dim`

**下一步（第四步）**我们会写一个**简单的策略网络**：

- 输入 `state`（长度 `state_dim`）
- 输出 `action`（长度 `action_dim`）

暂时**不**训练，只在环境里跑几步，看看「有一个策略」时交互长什么样。  
**第五步**再写价值网络。

---

## 五、小结

| 概念 | 输入 | 输出 | 作用 |
|------|------|------|------|
| **策略 (Policy)** | state | action | 决定「怎么做」 |
| **价值 (Value)** | state | 一个实数 | 评估「当前状态有多好」 |

- 策略 → 后面用 **Actor** 网络实现。
- 价值 → 后面用 **Critic** 网络实现。

理解到这儿，就可以进入 **第四步**：用 PyTorch 写一个**最小的策略网络 (Actor)**，在倒立摆里跑几步。
