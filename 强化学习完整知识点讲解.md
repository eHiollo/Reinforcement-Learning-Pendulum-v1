# ğŸ“ å¼ºåŒ–å­¦ä¹ å®Œæ•´çŸ¥è¯†ç‚¹è®²è§£
## åŸºäºä½ çš„ PPO é¡¹ç›®æ·±å…¥ç†è§£

---

## ğŸ“š ç›®å½•

1. [å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µ](#1-å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µ)
2. [ç¯å¢ƒäº¤äº’ä¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹](#2-ç¯å¢ƒäº¤äº’ä¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹)
3. [ç­–ç•¥ç½‘ç»œ (Actor)](#3-ç­–ç•¥ç½‘ç»œ-actor)
4. [ä»·å€¼ç½‘ç»œ (Critic)](#4-ä»·å€¼ç½‘ç»œ-critic)
5. [Actor-Critic æ¶æ„](#5-actor-critic-æ¶æ„)
6. [ç­–ç•¥æ¢¯åº¦æ–¹æ³•](#6-ç­–ç•¥æ¢¯åº¦æ–¹æ³•)
7. [PPO ç®—æ³•è¯¦è§£](#7-ppo-ç®—æ³•è¯¦è§£)
8. [GAE (å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡)](#8-gae-å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡)
9. [è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§](#9-è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§)
10. [é¡¹ç›®ä»£ç è§£æ](#10-é¡¹ç›®ä»£ç è§£æ)

---

## 1. å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µ

### 1.1 ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ï¼Ÿ

**å¼ºåŒ–å­¦ä¹  (Reinforcement Learning, RL)** æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ã€‚

**æ ¸å¿ƒè¦ç´ ï¼š**
- **æ™ºèƒ½ä½“ (Agent)**: åšå†³ç­–çš„å®ä½“ï¼ˆä½ çš„ç¥ç»ç½‘ç»œï¼‰
- **ç¯å¢ƒ (Environment)**: æ™ºèƒ½ä½“äº¤äº’çš„å¯¹è±¡ï¼ˆå€’ç«‹æ‘†ï¼‰
- **çŠ¶æ€ (State)**: ç¯å¢ƒçš„å½“å‰æƒ…å†µ
- **åŠ¨ä½œ (Action)**: æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ
- **å¥–åŠ± (Reward)**: ç¯å¢ƒå¯¹åŠ¨ä½œçš„åé¦ˆ
- **ç­–ç•¥ (Policy)**: ä»çŠ¶æ€åˆ°åŠ¨ä½œçš„æ˜ å°„

### 1.2 ä½ çš„é¡¹ç›®ä¸­çš„å¯¹åº”

```python
# step_01_explore_env.py
env = gym.make("Pendulum-v1")  # ç¯å¢ƒï¼šå€’ç«‹æ‘†
state = env.reset()            # çŠ¶æ€ï¼š3ç»´å‘é‡ [cos(Î¸), sin(Î¸), Î¸_dot]
action = env.action_space.sample()  # åŠ¨ä½œï¼š1ç»´è¿ç»­å€¼ [-2, 2]
next_state, reward, done, _ = env.step(action)  # å¥–åŠ±ï¼šæ ‡é‡
```

**çŠ¶æ€ç©ºé—´ (State Space)**:
- `state[0] = cos(Î¸)`: æ‘†è§’çš„ä½™å¼¦å€¼
- `state[1] = sin(Î¸)`: æ‘†è§’çš„æ­£å¼¦å€¼  
- `state[2] = Î¸_dot`: è§’é€Ÿåº¦

**åŠ¨ä½œç©ºé—´ (Action Space)**:
- è¿ç»­åŠ¨ä½œï¼šæ–½åŠ çš„åŠ›çŸ©ï¼ŒèŒƒå›´ `[-2, 2]`

**å¥–åŠ±å‡½æ•°**:
- è¶Šæ¥è¿‘ç«–ç›´å‘ä¸Šï¼ˆÎ¸=0ï¼‰ï¼Œå¥–åŠ±è¶Šé«˜
- è§’é€Ÿåº¦è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
- å¥–åŠ±èŒƒå›´é€šå¸¸åœ¨ `[-16.27, 0]`ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±

---

## 2. ç¯å¢ƒäº¤äº’ä¸é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹

### 2.1 é©¬å°”å¯å¤«æ€§è´¨ (Markov Property)

**å®šä¹‰**: æœªæ¥åªä¾èµ–äºå½“å‰çŠ¶æ€ï¼Œä¸å†å²æ— å…³ã€‚

```
P(S_{t+1} | S_t, A_t, S_{t-1}, A_{t-1}, ...) = P(S_{t+1} | S_t, A_t)
```

åœ¨ä½ çš„é¡¹ç›®ä¸­ï¼Œå€’ç«‹æ‘†çš„çŠ¶æ€ï¼ˆè§’åº¦ã€è§’é€Ÿåº¦ï¼‰å®Œå…¨å†³å®šäº†ä¸‹ä¸€æ­¥çš„çŠ¶æ€ï¼Œæ»¡è¶³é©¬å°”å¯å¤«æ€§è´¨ã€‚

### 2.2 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (MDP)

**MDP äº”å…ƒç»„**: (S, A, P, R, Î³)

- **S**: çŠ¶æ€ç©ºé—´
- **A**: åŠ¨ä½œç©ºé—´
- **P**: çŠ¶æ€è½¬ç§»æ¦‚ç‡ P(s'|s,a)
- **R**: å¥–åŠ±å‡½æ•° R(s,a,s')
- **Î³**: æŠ˜æ‰£å› å­ (gamma)

### 2.3 Episode ä¸ Return

**Episode (å›åˆ)**: ä»åˆå§‹çŠ¶æ€åˆ°ç»ˆæ­¢çŠ¶æ€çš„å®Œæ•´è½¨è¿¹ã€‚

```python
# step_06_collect_experience.py
def collect_experience(env, actor, buffer, device, max_steps=200, seed=None):
    state = env.reset(seed=seed)
    total_reward = 0.0
    
    for step in range(max_steps):
        action, log_prob = actor_select_action_with_log_prob(actor, state, device)
        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        
        buffer.add(state, action, reward, next_state, done, log_prob)
        total_reward += reward
        
        if done:
            break
```

**Return (å›æŠ¥)**: ä¸€ä¸ª episode çš„ç´¯ç§¯æŠ˜æ‰£å¥–åŠ±ã€‚

```
G_t = r_t + Î³*r_{t+1} + Î³Â²*r_{t+2} + ... = Î£_{k=0}^{âˆ} Î³^k * r_{t+k}
```

**æŠ˜æ‰£å› å­ Î³ (gamma)**:
- Î³ = 0: åªçœ‹å³æ—¶å¥–åŠ±
- Î³ = 1: æ‰€æœ‰æœªæ¥å¥–åŠ±ç­‰æƒé‡
- Î³ = 0.99 (å¸¸ç”¨): å¹³è¡¡å³æ—¶å’Œæœªæ¥å¥–åŠ±

---

## 3. ç­–ç•¥ç½‘ç»œ (Actor)

### 3.1 ç­–ç•¥ (Policy)

**ç­–ç•¥ Ï€(a|s)**: åœ¨çŠ¶æ€ s ä¸‹é€‰æ‹©åŠ¨ä½œ a çš„æ¦‚ç‡åˆ†å¸ƒã€‚

**ç¡®å®šæ€§ç­–ç•¥**: Ï€(s) = aï¼ˆç›´æ¥è¾“å‡ºåŠ¨ä½œï¼‰
**éšæœºæ€§ç­–ç•¥**: Ï€(a|s) = P(A_t=a | S_t=s)ï¼ˆè¾“å‡ºæ¦‚ç‡åˆ†å¸ƒï¼‰

### 3.2 ä½ çš„é¡¹ç›®ä¸­çš„ Actor

```python
# step_04_actor.py (ç®€å•ç‰ˆæœ¬)
class Actor(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden: int = 64):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fc3 = nn.Linear(hidden, action_dim)
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        action = torch.tanh(x) * 2.0  # é™åˆ¶åˆ° [-2, 2]
        return action
```

**é—®é¢˜**: è¿™ä¸ª Actor æ˜¯ç¡®å®šæ€§çš„ï¼Œæ— æ³•è®¡ç®—åŠ¨ä½œçš„æ¦‚ç‡ï¼ŒPPO éœ€è¦æ¦‚ç‡ï¼

### 3.3 PPO ç”¨çš„ Actor (å¸¦æ¦‚ç‡åˆ†å¸ƒ)

```python
# step_06_collect_experience.py
class PPOActor(nn.Module):
    def __init__(self, state_dim: int, action_dim: int, hidden: int = 64):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.mean_layer = nn.Linear(hidden, action_dim)      # è¾“å‡ºå‡å€¼
        self.log_std_layer = nn.Linear(hidden, action_dim)   # è¾“å‡ºæ ‡å‡†å·®çš„å¯¹æ•°
    
    def forward(self, state: torch.Tensor):
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        mean = self.mean_layer(x)
        log_std = torch.clamp(self.log_std_layer(x), min=-2, max=2)  # é˜²æ­¢ç­–ç•¥å´©æºƒ
        return mean, log_std
```

**å…³é”®ç‚¹**:
- **mean**: åŠ¨ä½œçš„å‡å€¼ï¼ˆç­–ç•¥çš„ä¸­å¿ƒï¼‰
- **log_std**: æ ‡å‡†å·®çš„å¯¹æ•°ï¼ˆæ§åˆ¶æ¢ç´¢ç¨‹åº¦ï¼‰
- **æ­£æ€åˆ†å¸ƒ**: Ï€(a|s) ~ N(mean, stdÂ²)
- **log_std é™åˆ¶**: `min=-2, max=2` ç¡®ä¿ `std âˆˆ [exp(-2), exp(2)] â‰ˆ [0.135, 7.39]`ï¼Œé˜²æ­¢ç­–ç•¥å´©æºƒ

### 3.4 åŠ¨ä½œé‡‡æ ·ä¸ log_prob

```python
# step_06_collect_experience.py
def actor_select_action_with_log_prob(actor: nn.Module, state: np.ndarray, device: torch.device):
    state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)
    
    with torch.no_grad():
        mean, log_std = actor(state_tensor)
        std = torch.exp(log_std)
        
        # ä»æ­£æ€åˆ†å¸ƒé‡‡æ ·
        normal = dist.Normal(mean, std)
        action_unbounded = normal.sample()
        log_prob = normal.log_prob(action_unbounded).sum(dim=-1)
        
        # é™åˆ¶åˆ° [-2, 2]ï¼ˆtanh å˜æ¢ï¼‰
        action = torch.tanh(action_unbounded) * 2.0
        
        # è°ƒæ•´ log_probï¼ˆå› ä¸ºç”¨äº† tanh å˜æ¢ï¼Œéœ€è¦é›…å¯æ¯”ä¿®æ­£ï¼‰
        log_prob -= torch.log(1 - torch.tanh(action_unbounded).pow(2) + 1e-6).sum(dim=-1)
        log_prob -= np.log(2.0)
    
    return action.squeeze(0).cpu().numpy(), log_prob.item()
```

**ä¸ºä»€ä¹ˆéœ€è¦ log_probï¼Ÿ**
- PPO æ›´æ–°æ—¶éœ€è¦è®¡ç®— `ratio = Ï€_new(a|s) / Ï€_old(a|s)`
- éœ€è¦çŸ¥é“æ—§ç­–ç•¥å’Œæ–°ç­–ç•¥ä¸‹é€‰æ‹©è¿™ä¸ªåŠ¨ä½œçš„æ¦‚ç‡

**tanh å˜æ¢çš„é›…å¯æ¯”ä¿®æ­£**:
- å¦‚æœ `a = tanh(x) * 2`ï¼Œé‚£ä¹ˆ `log P(a) = log P(x) - log(1-tanhÂ²(x)) - log(2)`
- è¿™æ˜¯æ¦‚ç‡å¯†åº¦å˜æ¢çš„æ•°å­¦è¦æ±‚

---

## 4. ä»·å€¼ç½‘ç»œ (Critic)

### 4.1 ä»·å€¼å‡½æ•°

**çŠ¶æ€ä»·å€¼å‡½æ•° V^Ï€(s)**: ä»çŠ¶æ€ s å¼€å§‹ï¼Œéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›å›æŠ¥ã€‚

```
V^Ï€(s) = E_Ï€[G_t | S_t = s] = E_Ï€[Î£_{k=0}^{âˆ} Î³^k * r_{t+k} | S_t = s]
```

**åŠ¨ä½œä»·å€¼å‡½æ•° Q^Ï€(s,a)**: åœ¨çŠ¶æ€ s æ‰§è¡ŒåŠ¨ä½œ aï¼Œç„¶åéµå¾ªç­–ç•¥ Ï€ çš„æœŸæœ›å›æŠ¥ã€‚

```
Q^Ï€(s,a) = E_Ï€[G_t | S_t = s, A_t = a]
```

**ä¼˜åŠ¿å‡½æ•° A^Ï€(s,a)**: åŠ¨ä½œ a ç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿ã€‚

```
A^Ï€(s,a) = Q^Ï€(s,a) - V^Ï€(s)
```

### 4.2 ä½ çš„é¡¹ç›®ä¸­çš„ Critic

```python
# step_05_critic.py
class Critic(nn.Module):
    def __init__(self, state_dim: int, hidden: int = 64):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.fc3 = nn.Linear(hidden, 1)  # è¾“å‡ºä¸€ä¸ªæ ‡é‡
    
    def forward(self, state: torch.Tensor) -> torch.Tensor:
        x = torch.relu(self.fc1(state))
        x = torch.relu(self.fc2(x))
        value = self.fc3(x)  # V(s)
        return value
```

**Critic çš„ä½œç”¨**:
1. **è¯„ä¼°çŠ¶æ€å¥½å**: V(s) å¤§ â†’ è¿™ä¸ªçŠ¶æ€å¥½
2. **è®¡ç®—ä¼˜åŠ¿**: A(s,a) = returns - V(s)
3. **å‡å°‘æ–¹å·®**: ç”¨ V(s) ä½œä¸º baselineï¼Œå‡å°‘ç­–ç•¥æ¢¯åº¦çš„æ–¹å·®

---

## 5. Actor-Critic æ¶æ„

### 5.1 ä¸ºä»€ä¹ˆéœ€è¦ Actor-Criticï¼Ÿ

**çº¯ç­–ç•¥æ¢¯åº¦ (REINFORCE)**:
- åªç”¨ Actorï¼Œä¸ç”¨ Critic
- é«˜æ–¹å·®ï¼Œè®­ç»ƒä¸ç¨³å®š
- éœ€è¦å¤§é‡æ ·æœ¬

**Actor-Critic**:
- Actor: å­¦ä¹ ç­–ç•¥ Ï€(a|s)
- Critic: å­¦ä¹ ä»·å€¼å‡½æ•° V(s)
- **ä¼˜åŠ¿**: ç”¨ V(s) ä½œä¸º baselineï¼Œå‡å°‘æ–¹å·®ï¼Œè®­ç»ƒæ›´ç¨³å®š

### 5.2 ä½ çš„é¡¹ç›®æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   State s   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚              â”‚
       â–¼              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Actor â”‚      â”‚ Critic â”‚
   â””â”€â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
       â”‚              â”‚
       â”‚              â”‚
       â–¼              â–¼
   Action a      Value V(s)
       â”‚              â”‚
       â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
              â–¼
         Advantage A(s,a)
              â”‚
              â–¼
         Update Actor & Critic
```

---

## 6. ç­–ç•¥æ¢¯åº¦æ–¹æ³•

### 6.1 ç­–ç•¥æ¢¯åº¦å®šç†

**ç›®æ ‡**: æœ€å¤§åŒ–æœŸæœ›å›æŠ¥ J(Î¸) = E_Ï€[G_0]

**ç­–ç•¥æ¢¯åº¦**:
```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) * Q^Ï€(s,a)]
```

**ç›´è§‚ç†è§£**:
- å¦‚æœ Q(s,a) > 0ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼Œå¢åŠ  Ï€(a|s)
- å¦‚æœ Q(s,a) < 0ï¼ˆååŠ¨ä½œï¼‰ï¼Œå‡å°‘ Ï€(a|s)
- æ¢¯åº¦å¤§å°ä¸ Q(s,a) æˆæ­£æ¯”

### 6.2 ç”¨ä¼˜åŠ¿å‡½æ•°æ›¿ä»£ Q

**é—®é¢˜**: Q^Ï€(s,a) éš¾ä»¥ä¼°è®¡

**è§£å†³**: ç”¨ä¼˜åŠ¿å‡½æ•° A^Ï€(s,a) = Q^Ï€(s,a) - V^Ï€(s)

```
âˆ‡_Î¸ J(Î¸) = E_Ï€[âˆ‡_Î¸ log Ï€_Î¸(a|s) * A^Ï€(s,a)]
```

**ä¸ºä»€ä¹ˆå¯ä»¥ï¼Ÿ**
- V^Ï€(s) ä¸ä¾èµ–äºåŠ¨ä½œ aï¼Œå¯¹æ¢¯åº¦æ²¡æœ‰è´¡çŒ®
- å‡å» V^Ï€(s) å¯ä»¥å‡å°‘æ–¹å·®ï¼ˆbaseline trickï¼‰

### 6.3 On-policy vs Off-policy

**On-policy (åŒç­–ç•¥)**:
- åªèƒ½ç”¨å½“å‰ç­–ç•¥ Ï€_Î¸ æ”¶é›†çš„æ•°æ®æ›´æ–°
- ä¾‹å­: REINFORCE, A2C, PPO

**Off-policy (å¼‚ç­–ç•¥)**:
- å¯ä»¥ç”¨æ—§ç­–ç•¥ Ï€_old æ”¶é›†çš„æ•°æ®æ›´æ–°
- ä¾‹å­: DQN, DDPG, SAC

**ä½ çš„é¡¹ç›®**: PPO æ˜¯ on-policyï¼Œä½†é€šè¿‡è£å‰ªè®©æ•°æ®èƒ½å¤ç”¨å‡ æ¬¡ï¼ˆä¼ª off-policyï¼‰

---

## 7. PPO ç®—æ³•è¯¦è§£

### 7.1 ä¸ºä»€ä¹ˆéœ€è¦ PPOï¼Ÿ

**é—®é¢˜**: ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ¯æ¬¡æ›´æ–°éƒ½è¦ç”¨æ–°æ•°æ®ï¼Œæ ·æœ¬æ•ˆç‡ä½ã€‚

**PPO çš„è§£å†³æ–¹æ¡ˆ**: é€šè¿‡**è£å‰ª (clipping)** é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œè®©æ—§æ•°æ®èƒ½å¤ç”¨å‡ æ¬¡ã€‚

### 7.2 PPO ç›®æ ‡å‡½æ•°

**åŸå§‹ç­–ç•¥æ¢¯åº¦**:
```
L^PG(Î¸) = E_t[log Ï€_Î¸(a_t|s_t) * A_t]
```

**PPO è£å‰ªç›®æ ‡**:
```
L^CLIP(Î¸) = E_t[min(
    r_t(Î¸) * A_t,                    # æœªè£å‰ª
    clip(r_t(Î¸), 1-Îµ, 1+Îµ) * A_t     # è£å‰ªå
)]
```

å…¶ä¸­ `r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)` æ˜¯é‡è¦æ€§é‡‡æ ·æ¯”ç‡ã€‚

### 7.3 ä½ çš„é¡¹ç›®ä¸­çš„å®ç°

```python
# step_07_ppo_update.py
def ppo_update(...):
    # 1. è®¡ç®—æ–°ç­–ç•¥çš„ log_prob
    new_log_probs = compute_log_prob(actor, states, actions)
    
    # 2. è®¡ç®— ratio = Ï€_new / Ï€_old
    log_ratio = new_log_probs - old_log_probs
    log_ratio = torch.clamp(log_ratio, min=-10.0, max=10.0)  # é˜²æ­¢æº¢å‡º
    ratio = torch.exp(log_ratio)
    
    # 3. PPO è£å‰ª
    surr1 = ratio * advantages                    # æœªè£å‰ª
    surr2 = torch.clamp(ratio, 1.0 - eps_clip, 1.0 + eps_clip) * advantages  # è£å‰ª
    actor_loss = -torch.min(surr1, surr2).mean()  # å– minï¼Œé˜²æ­¢è¿‡åº¦ä¼˜åŒ–
    
    # 4. æ·»åŠ ç†µå¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
    actor_loss = actor_loss - entropy_coef * entropy
    
    # 5. Critic æŸå¤±
    values_pred = critic(states).squeeze(-1)
    critic_loss = F.smooth_l1_loss(values_pred, returns) * value_coef
```

**å…³é”®ç‚¹**:
- **eps_clip = 0.2**: é™åˆ¶ ratio åœ¨ [0.8, 1.2] ä¹‹é—´
- **å– min**: é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
- **å¤šè½®æ›´æ–° (k_epochs)**: å¯¹åŒä¸€æ‰¹æ•°æ®æ›´æ–°å¤šæ¬¡ï¼Œæé«˜æ ·æœ¬æ•ˆç‡

### 7.4 PPO çš„ä¸¤ç§å˜ä½“

**PPO-Clip** (ä½ çš„é¡¹ç›®ç”¨çš„):
- ç”¨è£å‰ªé™åˆ¶æ›´æ–°å¹…åº¦

**PPO-Penalty**:
- ç”¨ KL æ•£åº¦æƒ©ç½šé¡¹é™åˆ¶æ›´æ–°å¹…åº¦

---

## 8. GAE (å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡)

### 8.1 ä¸ºä»€ä¹ˆéœ€è¦ GAEï¼Ÿ

**é—®é¢˜**: å¦‚ä½•ä¼°è®¡ä¼˜åŠ¿å‡½æ•° A^Ï€(s,a)ï¼Ÿ

**æ–¹æ³•1: è’™ç‰¹å¡æ´› (MC)**:
```
A_t = G_t - V(s_t)  # G_t æ˜¯å®é™…å›æŠ¥
```
- ä¼˜ç‚¹: æ— å
- ç¼ºç‚¹: é«˜æ–¹å·®

**æ–¹æ³•2: TD (æ—¶åºå·®åˆ†)**:
```
A_t = r_t + Î³*V(s_{t+1}) - V(s_t)  # TD è¯¯å·®
```
- ä¼˜ç‚¹: ä½æ–¹å·®
- ç¼ºç‚¹: æœ‰å

**æ–¹æ³•3: GAE (å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡)**:
- ç»“åˆ MC å’Œ TDï¼Œå¹³è¡¡åå·®å’Œæ–¹å·®

### 8.2 GAE å…¬å¼

**TD è¯¯å·®**:
```
Î´_t = r_t + Î³*V(s_{t+1}) - V(s_t)
```

**GAE**:
```
A_t^GAE = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...
```

**å‚æ•° Î» (gae_lambda)**:
- Î» = 0: çº¯ TDï¼ˆä½æ–¹å·®ï¼Œé«˜åå·®ï¼‰
- Î» = 1: çº¯ MCï¼ˆé«˜æ–¹å·®ï¼Œä½åå·®ï¼‰
- Î» = 0.95 (å¸¸ç”¨): æŠ˜ä¸­

### 8.3 ä½ çš„é¡¹ç›®ä¸­çš„å®ç°

```python
# step_07_ppo_update.py
def compute_gae(rewards, values, next_values, dones, gamma=0.99, gae_lambda=0.95):
    T = len(rewards)
    advantages = np.zeros(T, dtype=np.float32)
    gae = 0.0
    
    # ä»åå¾€å‰è®¡ç®— GAE
    for t in reversed(range(T)):
        mask = 1.0 - float(dones[t])  # å¦‚æœ doneï¼Œä¸‹ä¸€çŠ¶æ€å€¼ä¸º 0
        
        # TD è¯¯å·®
        delta = rewards[t] + gamma * next_values[t] * mask - values[t]
        
        # GAE ç´¯åŠ 
        gae = delta + gamma * gae_lambda * mask * gae
        advantages[t] = gae
    
    returns = advantages + values  # ç”¨äºè®­ç»ƒ Critic
    return advantages, returns
```

**å…³é”®ç‚¹**:
- **ä»åå¾€å‰è®¡ç®—**: GAE ä¾èµ–äºæœªæ¥çš„ TD è¯¯å·®
- **mask**: å¦‚æœ `done=True`ï¼Œä¸‹ä¸€çŠ¶æ€å€¼ä¸º 0ï¼ŒGAE ä¸ä¼ é€’

### 8.4 å‘é‡åŒ–ç¯å¢ƒçš„ GAE

**é—®é¢˜**: å½“ä½¿ç”¨å¹¶è¡Œç¯å¢ƒæ—¶ï¼Œæ•°æ®æ˜¯æŒ‰ step å­˜å‚¨çš„ï¼Œä½† GAE éœ€è¦æŒ‰è½¨è¿¹è®¡ç®—ã€‚

**è§£å†³**: `compute_gae_vectorized` å‡½æ•°é‡å¡‘æ•°æ®ï¼Œä¸ºæ¯ä¸ªç¯å¢ƒç‹¬ç«‹è®¡ç®— GAEã€‚

```python
# step_07_ppo_update.py
def compute_gae_vectorized(rewards, values, next_values, dones, 
                           gamma=0.99, gae_lambda=0.95, num_envs=1):
    # é‡å¡‘ä¸º (num_steps, num_envs)
    rewards = rewards.reshape(num_steps, num_envs)
    values = values.reshape(num_steps, num_envs)
    # ...
    
    # ä¸ºæ¯ä¸ªç¯å¢ƒç‹¬ç«‹è®¡ç®— GAE
    for t in reversed(range(num_steps)):
        mask = 1.0 - dones[t].astype(np.float32)
        delta = rewards[t] + gamma * next_values[t] * mask - values[t]
        gae = delta + gamma * gae_lambda * mask * gae
        advantages[t] = gae
```

---

## 9. è®­ç»ƒæŠ€å·§ä¸ç¨³å®šæ€§

### 9.1 ä¼˜åŠ¿å½’ä¸€åŒ–

**ä¸ºä»€ä¹ˆå½’ä¸€åŒ–ï¼Ÿ**
- ä¼˜åŠ¿çš„å°ºåº¦å¯èƒ½å¾ˆå¤§ï¼Œå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®š
- å½’ä¸€åŒ–åï¼Œä¼˜åŠ¿å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1

```python
# step_07_ppo_update.py
adv_mean = advantages.mean()
adv_std = advantages.std()
advantages = (advantages - adv_mean) / (adv_std + 1e-8)
```

### 9.2 ç†µå¥–åŠ± (Entropy Bonus)

**ç†µ**: è¡¡é‡ç­–ç•¥çš„éšæœºæ€§
```
H(Ï€) = -Î£_a Ï€(a|s) log Ï€(a|s)
```

**é«˜ç†µ**: ç­–ç•¥éšæœºï¼Œæ¢ç´¢å¤š
**ä½ç†µ**: ç­–ç•¥ç¡®å®šï¼Œåˆ©ç”¨å¤š

**ç†µå¥–åŠ±**: åœ¨ loss ä¸­å‡å»ç†µï¼Œé¼“åŠ±æ¢ç´¢
```python
actor_loss = actor_loss - entropy_coef * entropy
```

### 9.3 KL æ•£åº¦ç›‘æ§

**KL æ•£åº¦**: è¡¡é‡æ–°æ—§ç­–ç•¥çš„å·®å¼‚
```
KL(Ï€_old || Ï€_new) = E[log Ï€_old - log Ï€_new]
```

**ä½œç”¨**:
- ç›‘æ§ç­–ç•¥å˜åŒ–
- å¦‚æœ KL å¤ªå¤§ï¼ˆ>0.02ï¼‰ï¼Œæå‰åœæ­¢æ›´æ–°ï¼Œé˜²æ­¢ç ´åå·²å­¦åˆ°çš„çŸ¥è¯†

```python
# step_07_ppo_update.py
kl_div = ((torch.exp(log_ratio) - 1) - log_ratio).mean().item()
if kl_div > 0.02:
    break  # æå‰åœæ­¢
```

### 9.4 æ¢¯åº¦è£å‰ª

**ç›®çš„**: é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

```python
# step_07_ppo_update.py
torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm=0.5)
torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm=0.5)
```

### 9.5 NaN/Inf æ£€æµ‹

**é‡è¦æ€§**: è®­ç»ƒä¸­å‡ºç° NaN ä¼šå¯¼è‡´æ¨¡å‹å´©æºƒ

**æ£€æµ‹ç‚¹**:
- advantages å½’ä¸€åŒ–å
- returns è®¡ç®—å
- loss è®¡ç®—å
- æ¢¯åº¦è®¡ç®—å

```python
# step_07_ppo_update.py
if torch.any(torch.isnan(advantages)) or torch.any(torch.isinf(advantages)):
    print("è­¦å‘Šï¼šadvantageså¼‚å¸¸ï¼Œé‡æ–°è®¡ç®—")
    # å¤„ç†é€»è¾‘...
```

### 9.6 å­¦ä¹ ç‡è®¾ç½®

**Actor å’Œ Critic é€šå¸¸ç”¨ä¸åŒçš„å­¦ä¹ ç‡**:
- `lr_actor = 1e-4`: ç­–ç•¥æ›´æ–°è¦è°¨æ…
- `lr_critic = 3e-4`: å€¼å‡½æ•°å¯ä»¥å­¦å¾—å¿«ä¸€ç‚¹

**åŸå› **: å¦‚æœ Critic å­¦å¾—å¤ªæ…¢ï¼Œä¼˜åŠ¿ä¼°è®¡ä¸å‡†ï¼›å¦‚æœ Actor å­¦å¾—å¤ªå¿«ï¼Œç­–ç•¥å¯èƒ½å´©æºƒã€‚

### 9.7 æŸå¤±å‡½æ•°é€‰æ‹©

**Critic Loss**:
- **MSE**: `F.mse_loss(values_pred, returns)`
- **Huber Loss** (ä½ çš„é¡¹ç›®ç”¨çš„): `F.smooth_l1_loss(values_pred, returns)`
  - å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ï¼Œé˜²æ­¢è®­ç»ƒä¸ç¨³å®š

---

## 10. é¡¹ç›®ä»£ç è§£æ

### 10.1 å®Œæ•´è®­ç»ƒæµç¨‹

```python
# step_08_train.py
def train(...):
    # 1. åˆå§‹åŒ–
    actor = PPOActor(...)
    critic = Critic(...)
    opt_actor = optim.Adam(...)
    opt_critic = optim.Adam(...)
    
    # 2. è®­ç»ƒå¾ªç¯
    for episode in range(max_episodes):
        # 2.1 æ”¶é›†ç»éªŒ
        buffer.clear()
        collect_experience_vector(envs, actor, buffer, ...)
        
        # 2.2 PPO æ›´æ–°
        actor_loss, critic_loss, entropy, kl_div, mean_adv = \
            ppo_update(actor, critic, buffer, ...)
        
        # 2.3 è®°å½•æ—¥å¿—
        writer.add_scalar('Reward/Episode', episode_reward, episode)
        writer.add_scalar('Loss/Actor', actor_loss, episode)
        # ...
```

### 10.2 å¹¶è¡Œç¯å¢ƒ

**ä¸ºä»€ä¹ˆç”¨å¹¶è¡Œç¯å¢ƒï¼Ÿ**
- åŠ é€Ÿæ•°æ®æ”¶é›†
- æé«˜æ ·æœ¬å¤šæ ·æ€§

```python
# step_08_train.py
from gymnasium.vector import SyncVectorEnv

envs = SyncVectorEnv([lambda: PendulumWrapper() for _ in range(num_envs)])
```

**æ³¨æ„**: GAE è®¡ç®—éœ€è¦æ­£ç¡®å¤„ç†å¹¶è¡Œç¯å¢ƒçš„æ•°æ®ç»“æ„ã€‚

### 10.3 ç»éªŒç¼“å†²åŒº

```python
# step_06_collect_experience.py
class ExperienceBuffer:
    def __init__(self):
        self.states = []
        self.actions = []
        self.rewards = []
        self.next_states = []  # ç”¨äº GAE
        self.dones = []
        self.log_probs = []
```

**å­˜å‚¨å†…å®¹**:
- `(s, a, r, s', done, log_prob)`: å®Œæ•´çš„ transition

### 10.4 æ£€æŸ¥ç‚¹ (Checkpoint)

**ç›®çš„**: ä¿å­˜æ¨¡å‹ï¼Œå¯ä»¥æ¢å¤è®­ç»ƒ

```python
# step_08_train.py
def save_model(actor, critic, opt_actor, opt_critic, episode, ...):
    checkpoint = {
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'episode': episode,
        'episode_rewards': episode_rewards,
        # ...
    }
    torch.save(checkpoint, filepath)
```

---

## ğŸ“Š å…³é”®è¶…å‚æ•°æ€»ç»“

| è¶…å‚æ•° | é»˜è®¤å€¼ | ä½œç”¨ | è°ƒå‚å»ºè®® |
|--------|--------|------|----------|
| `gamma` | 0.99 | æŠ˜æ‰£å› å­ | 0.95-0.999 |
| `gae_lambda` | 0.95 | GAE å‚æ•° | 0.9-0.99 |
| `eps_clip` | 0.2 | PPO è£å‰ªèŒƒå›´ | 0.1-0.3 |
| `k_epochs` | 4 | å¤šè½®æ›´æ–°æ¬¡æ•° | 3-10 |
| `lr_actor` | 1e-4 | Actor å­¦ä¹ ç‡ | 1e-5 åˆ° 3e-4 |
| `lr_critic` | 3e-4 | Critic å­¦ä¹ ç‡ | 1e-4 åˆ° 1e-3 |
| `entropy_coef` | 0.01 | ç†µå¥–åŠ±ç³»æ•° | 0.001-0.1 |
| `value_coef` | 1.0 | Critic æŸå¤±æƒé‡ | 0.5-2.0 |
| `max_grad_norm` | 0.5 | æ¢¯åº¦è£å‰ªé˜ˆå€¼ | 0.5-1.0 |

---

## ğŸ¯ å­¦ä¹ è¦ç‚¹æ€»ç»“

1. **å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒ**: é€šè¿‡è¯•é”™å­¦ä¹ æœ€ä¼˜ç­–ç•¥
2. **MDP æ¡†æ¶**: çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±ã€è½¬ç§»æ¦‚ç‡
3. **Actor-Critic**: Actor å­¦ç­–ç•¥ï¼ŒCritic å­¦ä»·å€¼ï¼Œäº’ç›¸ä¿ƒè¿›
4. **PPO**: é€šè¿‡è£å‰ªé™åˆ¶æ›´æ–°ï¼Œæé«˜æ ·æœ¬æ•ˆç‡
5. **GAE**: å¹³è¡¡åå·®å’Œæ–¹å·®ï¼Œç¨³å®šä¼˜åŠ¿ä¼°è®¡
6. **ç¨³å®šæ€§æŠ€å·§**: å½’ä¸€åŒ–ã€æ¢¯åº¦è£å‰ªã€NaN æ£€æµ‹ã€KL ç›‘æ§

---

## ğŸ“š è¿›ä¸€æ­¥å­¦ä¹ 

- **è®ºæ–‡**: "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
- **ä¹¦ç±**: "Reinforcement Learning: An Introduction" (Sutton & Barto)
- **å…¶ä»–ç®—æ³•**: DDPG, SAC, TD3, TRPO

---

**æ­å–œä½ å®Œæˆäº†è¿™ä¸ªå®Œæ•´çš„ PPO é¡¹ç›®ï¼** ğŸ‰

é€šè¿‡è¿™ä¸ªé¡¹ç›®ï¼Œä½ ä¸ä»…å®ç°äº† PPO ç®—æ³•ï¼Œè¿˜æ·±å…¥ç†è§£äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µã€‚ç»§ç»­æ¢ç´¢ï¼Œå°è¯•ä¸åŒçš„ç¯å¢ƒå’Œç®—æ³•å§ï¼

